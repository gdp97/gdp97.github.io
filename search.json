[{"title":"Transformer","path":"/2023/01/12/Transformer/","content":"1-1 æ‘˜è¦è½¬å½•æ¨¡å‹ï¼šç”¨ä¸€ä¸ªåºåˆ—ç”Ÿæˆå¦ä¸€ä¸ªåºåˆ—ã€‚ä¾èµ–ä½¿ç”¨encoder-decoderæ¶æ„çš„å¤æ‚çš„RNNæˆ–CNNã€‚åœ¨encoderå’Œdecoderä¹‹é—´ä½¿ç”¨attentionæœºåˆ¶ã€‚ å‰ä¸¤å¥è¯´å½“å‰çš„åºåˆ—è½¬å½•æ¨¡å‹æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œç¬¬ä¸‰å¥è¯´æå‡ºäº†ä¸€ä¸ªæ–°çš„ç®€å•çš„ç½‘ç»œæ¶æ„ï¼šTransformerï¼Œä»…ä¾èµ–attentionæœºåˆ¶ï¼Œæ²¡æœ‰CNNå’ŒRNNã€‚æ¥ä¸‹æ¥è®²ï¼Œåœ¨ä¸€äº›æœºå™¨ç¿»è¯‘å®éªŒä¸Šç»“æœæ¯”å…¶ä»–çš„æ¨¡å‹æ•ˆæœå¥½ã€‚ 1-2 ç»“è®ºæ˜¯ç¬¬ä¸€ä¸ªä»…ä½¿ç”¨attentionæœºåˆ¶çš„è½¬å½•æ¨¡å‹ï¼Œå°†ä¹‹å‰æ‰€æœ‰çš„å¾ªç¯å±‚å…¨éƒ¨æ¢æˆäº†multi-headed self-attentionã€‚ å°†è¯¥æ¨¡å‹ä½¿ç”¨åœ¨æœºå™¨ç¿»è¯‘ä¹‹å¤–çš„é¢†åŸŸï¼Œå¦‚å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ï¼›æˆ–è€…ä½¿ç”Ÿæˆå˜å¾—æ›´å°‘æ—¶åºæ€§(generation less sequential) 1-3 å¼•è¨€åœ¨æ—¶åºæ¨¡å‹å’Œè½¬å½•é—®é¢˜ä¸­(è¯­è¨€æ¨¡å‹å’Œæœºå™¨ç¿»è¯‘)ï¼Œæœ€å¸¸ç”¨çš„æ˜¯RNN(åŒ…æ‹¬LSTMã€GRU)ã€‚å…¶ä¸­æœ‰ä¸¤ä¸ªä¸»è¦çš„æ¨¡å‹â€”â€”å¾ªç¯è¯­è¨€æ¨¡å‹å’Œencoder-decoderæ¶æ„(å½“è¾“å‡ºç»“æ„åŒ–ä¿¡æ¯æ¯”è¾ƒå¤šçš„æ—¶å€™ä½¿ç”¨)ã€‚ RNNçš„ç‰¹ç‚¹å’Œç¼ºç‚¹ï¼šä¸€ä¸ªåºåˆ—çš„è®¡ç®—æ˜¯ä»å·¦å¾€å³ä¸€æ­¥ä¸€æ­¥åœ°åšã€‚å‡è®¾åºåˆ—ä¸ºå¥å­ï¼Œåˆ™è¾“å…¥æ˜¯ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°æ¥ç€ï¼Œå¯¹ç¬¬tä¸ªè¯è¾“å‡ºä¸€ä¸ªéšè—çŠ¶æ€htï¼Œhtæ˜¯ç”±å‰ä¸€ä¸ªéšè—çŠ¶æ€å’Œå½“å‰ç¬¬tä¸ªè¯æœ¬èº«å†³å®šçš„ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥å°†ä¹‹å‰å­¦åˆ°çš„å†å²ä¿¡æ¯ç”±ht-1æ”¾åˆ°è¿™ä¸ªhtä¸­ã€‚ç¼ºç‚¹æ˜¯ç”±äºæ˜¯æ—¶åºï¼Œä¸€æ­¥ä¸€æ­¥è®¡ç®—çš„è¿‡ç¨‹ï¼Œéš¾ä»¥å¹¶è¡Œã€‚å¯¹äºé•¿åºåˆ—å¾ˆä¸å‹å¥½ï¼Œå†…å­˜é™åˆ¶äº†ä»–çš„æ‰¹å¤„ç†ã€‚ attentionä¸RNNç»“åˆä½¿ç”¨ï¼ŒæŠŠencoderçš„ä¸œè¥¿æœ‰æ•ˆä¼ ç»™decoderã€‚ æå‡ºTransformerï¼Œä¸ä½¿ç”¨RNNï¼Œæ˜¯çº¯attentionæœºåˆ¶ï¼Œæ˜¯å¯ä»¥å¹¶è¡Œè¿ç®—çš„ã€‚åœ¨æ›´çŸ­çš„æ—¶é—´å†…ï¼Œåšåˆ°æ¯”ä¹‹å‰æ›´å¥½çš„ç»“æœã€‚ 1-4 ç›¸å…³å·¥ä½œ(background)ä½¿ç”¨CNNä½œä¸ºåŸºæœ¬æ„ä»¶å—ï¼Œæ›¿æ¢RNNå‡å°‘æ—¶åºè®¡ç®—ï¼šä½†æ˜¯CNNå¯¹äºé•¿åºåˆ—æ¨¡å‹éš¾ä»¥å»ºæ¨¡ï¼Œå› ä¸ºä¸¤ä¸ªåƒç´ ç‚¹å¾ˆè¿œçš„è¯ï¼Œéœ€è¦å¾ˆå¤šå±‚å·ç§¯ï¼Œæ‰èƒ½å°†éš”å¾—è¿œçš„ä¸¤ä¸ªåƒç´ èåˆèµ·æ¥ã€‚Transformerä¸­çš„attentionæœºåˆ¶å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„åƒç´ ï¼Œä¸€å±‚å°±å¯ä»¥å°†æ•´ä¸ªåºåˆ—çœ‹åˆ°ã€‚å·ç§¯å¯ä»¥æœ‰å¤šä¸ªè¾“å‡ºé€šé“ï¼Œæ¯ä¸ªè¾“å‡ºé€šé“å¯ä»¥è¯†åˆ«ä¸ä¸€æ ·çš„æ¨¡å¼ã€‚æå‡ºäº†ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(Multi-Head Attention) è‡ªæ³¨æ„åŠ›æœºåˆ¶(Self-Attention)çš„æ¦‚å¿µ memory-networkçš„æ¦‚å¿µ æ˜¯ç¬¬ä¸€ä¸ªåªä½¿ç”¨attentionçš„æ¨¡å‹ 1-5 æ¨¡å‹åºåˆ—æ¨¡å‹ä¸­æ¯”è¾ƒå¥½çš„æ˜¯ä¸€ä¸ªå«åšç¼–ç å™¨å’Œè§£ç å™¨çš„æ¶æ„ã€‚ä»€ä¹ˆæ˜¯ç¼–ç å™¨å’Œè§£ç å™¨ï¼Ÿç¼–ç å™¨æ˜¯æŒ‡å°†ä¸€ä¸ªè¾“å…¥åºåˆ—$$(x_1,â€¦,x_n)$$è½¬æ¢æˆå¯¹åº”çš„å‘é‡çš„è¡¨ç¤º$$z=(z_1,â€¦,z_n)$$ã€‚ä¹Ÿå°±æ˜¯ç¼–ç å™¨å°†ä¸€äº›åŸå§‹çš„æ•°æ®è½¬æ¢æˆæœºå™¨å­¦ä¹ å¯ä»¥ç†è§£çš„ä¸€ç³»åˆ—å‘é‡ã€‚è§£ç å™¨æ˜¯å°†ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œç„¶åç”Ÿæˆä¸€ä¸ª(ä¸€ä¸ªæ—¶é—´åªæœ‰ä¸€ä¸ªå…ƒç´ )åºåˆ—$$(y_1,â€¦,y_m)$$ï¼Œå…¶ä¸­nå’Œmä¸ä¸€å®šç›¸ç­‰ã€‚æ¯ç”Ÿæˆä¸€ä¸ª$$y_i$$(è¾“å‡º)ä¹Ÿä¼šä½œä¸ºä¸‹ä¸€æ—¶åˆ»çš„è¾“å…¥ï¼Œæ¥ç”Ÿæˆä¸‹ä¸€æ—¶åˆ»çš„è¾“å‡ºï¼Œè¿™å«åšè‡ªå›å½’æ¨¡å‹ã€‚ transformeræ˜¯å°†ä¸€äº›è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œé€ç‚¹æ–¹å¼(point-wise)ï¼Œå…¨è¿æ¥å±‚ç”¨ç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„æ¥å †åœ¨ä¸€èµ·çš„ã€‚ 1-5-1 ç¼–è§£ç å™¨ç¼–ç å™¨ï¼šæ€»å…±ä½¿ç”¨äº†6ä¸ªå®Œå…¨ä¸€æ ·çš„å±‚ã€‚æ¯ä¸ªå±‚ä¸­æœ‰ä¸¤ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›(multi-head self-attention)ã€åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ(positionwise feed-forward network)â€”â€”å¤šå±‚æ„ŸçŸ¥æœºã€‚æ¯ä¸ªå­å±‚éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥ï¼Œåœ¨æ®‹å·®è¿æ¥çš„åŠ æ³•è®¡ç®—ä¹‹åï¼Œè·Ÿç€ä¸€ä¸ªå±‚è§„èŒƒåŒ–(layer normalization)ã€‚å…¬å¼ä¸º$$LayerNorm(x+Sublayer(x))$$ã€‚ä¸ºäº†ç®€åŒ–æ®‹å·®è¿æ¥ï¼Œæ‰€ä»¥ä½¿æ‰€æœ‰å­å±‚çš„è¾“å‡ºçš„ç»´åº¦å˜æˆ512ã€‚ æ³¨ï¼šbatch normalizationå’Œlayer normalizationçš„åŒºåˆ« æ³¨ï¼šè“è‰²çº¿æ˜¯batch normalizationï¼Œé»„è‰²çº¿æ˜¯layer normalizationã€‚ å½“æ•°æ®æ˜¯äºŒç»´æ—¶(batch, feature)ï¼Œbatch normæ˜¯åœ¨featureç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼›layer normæ˜¯åœ¨batchç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–(å³æ¯ä¸ªæ ·æœ¬è‡ªèº«è¿›è¡Œå½’ä¸€åŒ–å¤„ç†)ã€‚ å½“æ•°æ®æ˜¯ä¸‰ç»´æ—¶(batch, sequence, feature)ï¼Œbatch normæ˜¯åœ¨featureç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼›layer normæ˜¯åœ¨batchç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–(å³æ¯ä¸ªæ ·æœ¬è‡ªèº«è¿›è¡Œå½’ä¸€åŒ–å¤„ç†)ã€‚æ¯ä¸ªæ ·æœ¬çš„åºåˆ—é•¿åº¦ä¸ä¸€æ ·é•¿ï¼Œå¦‚æœæ ·æœ¬é•¿åº¦å˜åŒ–è¾ƒå¤§æ—¶ç”šè‡³æµ‹è¯•æ—¶å‡ºç°æ›´é•¿çš„æ ·æœ¬ï¼Œbatch normä¸å¤ªé€‚ç”¨è¿›è¡Œå½’ä¸€ã€‚layer normæ˜¯å¯¹æ ·æœ¬è‡ªèº«è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¯”è¾ƒé€‚åˆã€‚ ç¼–ç å™¨ï¼šæ€»å…±ä½¿ç”¨äº†6ä¸ªå®Œå…¨ä¸€æ ·çš„å±‚ã€‚æ¯ä¸ªå±‚ä¸­æœ‰ä¸‰ä¸ªå­å±‚ï¼šå¸¦æ©ç çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¯ä»¥çœ‹åˆ°å…¨éƒ¨çš„è¾“å…¥ï¼Œå¸¦æ©ç æ˜¯ä¿è¯åœ¨tæ—¶é—´ä¸ä¼šçœ‹åˆ°tæ—¶é—´ä»¥åçš„è¾“å…¥ï¼Œä»è€Œä¿è¯è®­ç»ƒå’Œé¢„æµ‹çš„æ—¶å€™è¡Œä¸ºæ˜¯ä¸€è‡´çš„)ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€å¤šå±‚æ„ŸçŸ¥æœºã€‚ 1-5-2 æ³¨æ„åŠ›å±‚æ³¨æ„åŠ›å‡½æ•°æ˜¯å°†ä¸€ä¸ªqueryå’Œä¸€äº›key-valueå¯¹æ˜ å°„æˆä¸€ä¸ªè¾“å‡ºçš„å‡½æ•°ï¼Œè¿™é‡Œæ‰€æœ‰çš„queryã€keysã€valuesã€outputéƒ½æ˜¯ä¸€äº›å‘é‡ã€‚outputæ˜¯valueçš„åŠ æƒå’Œï¼Œæ‰€ä»¥outputçš„ç»´åº¦å’Œvalueçš„ç»´åº¦æ˜¯ä¸€æ ·çš„ã€‚ å¯¹åº”æ¯ä¸€ä¸ªvalueçš„æƒé‡ï¼Œæ˜¯ç”±valueå¯¹åº”çš„key-queryçš„ç›¸ä¼¼åº¦(compatibility function)è®¡ç®—æ¥çš„ã€‚ 1-5-2-1 Scaled Dot-Product AttentionScaled Dot-Product Attentionï¼Œæ˜¯ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚è¾“å…¥æ˜¯ç”±$$d_k$$é•¿åº¦çš„querieså’Œkeyså’Œ$$d_v$$é•¿åº¦çš„valuesç»„æˆçš„(ä¸è¾“å‡ºçš„é•¿åº¦ç›¸ç­‰)ã€‚å¯¹queryå’Œæ‰€æœ‰çš„keyä¾æ¬¡è¿›è¡Œå†…ç§¯ï¼Œç„¶åé™¤ä»¥$$\\sqrt{d_k}$$ï¼Œç„¶åå†é€šè¿‡softmaxå‡½æ•°å°±å¯ä»¥å¾—åˆ°nä¸ªéè´Ÿçš„è€Œä¸”åŠ èµ·æ¥å’Œç­‰äºä¸€çš„ä¸€ä¸ªæƒé‡ï¼Œå°†æƒé‡ä½œç”¨åˆ°valueä¸Šå°±å¯ä»¥å¾—åˆ°ç›¸åº”çš„è¾“å‡ºã€‚å…·ä½“å¦‚ä¸‹ï¼šä½¿ç”¨ä¸¤æ¬¡çŸ©é˜µä¹˜æ³• æœ€å¸¸è§çš„ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ï¼šadditive attention(åŠ å‹æ³¨æ„åŠ›)-ç”¨äºå¤„ç†queryå’Œkeyä¸ç­‰é•¿çš„æƒ…å†µã€dot-product(multi-plicative) attention(ç‚¹ç§¯çš„æ³¨æ„åŠ›æœºåˆ¶)ã€‚å½“$$d_k$$è¾ƒå¤§æ—¶ï¼Œqueryå’Œkeyç‚¹ç§¯çš„ç»“æœæ›´å¤§çš„å€¼ï¼Œå…¶softmaxçš„ç»“æœæ›´è¶‹å‘äº1ï¼Œå…¶ä»–ç»“æœè¾ƒå°å€¼çš„softmaxçš„ç»“æœæ›´è¶‹å‘äº0ã€‚è¿™æ ·å°±ä¼šå¯¼è‡´æ¢¯åº¦æ¯”è¾ƒå°ï¼Œä½¿æ¨¡å‹è®¤ä¸ºå·²ç»æ¥è¿‘æ”¶æ•›ï¼ŒLOSSä¸‹é™ä¼šå˜æ…¢ï¼Œæ‰€ä»¥è¿›è¡Œäº†é™¤ä»¥$$\\sqrt{d_k}$$çš„æ“ä½œã€‚ Maskedçš„ä½œç”¨ï¼šæ™®é€šçš„æ³¨æ„åŠ›æœºåˆ¶$$Q$$ä¼šå’Œæ‰€æœ‰çš„$$K$$è¿›è¡Œç‚¹ä¹˜è¿ç®—ï¼Œè€Œè¿™é‡Œçš„éœ€è¦å°†$$t$$æ—¶é—´ä¹‹åçš„$$K$$å±è”½æ‰ã€‚åšæ³•å°±æ˜¯å°†$$t$$ä»¥åŠ$$t$$æ—¶é—´ä¹‹åçš„$$Qã€K$$ç‚¹ä¹˜è¿ç®—çš„å€¼æ›¿æ¢æˆéå¸¸å¤§çš„è´Ÿæ•°(-1e9)ï¼Œå†ç»è¿‡softmaxè¿ç®—å¾—åˆ°çš„ç»“æœå°±ä¼šå˜æˆé›¶ï¼Œå³$$t$$ä»¥åŠ$$t$$æ—¶é—´ä¹‹åçš„Væ‰€å¯¹åº”çš„æƒé‡éƒ½ä¸ºé›¶ã€‚å› æ­¤ï¼Œè®¡ç®—è¾“å‡ºçš„æ—¶å€™åªç”¨åˆ°äº†$$(V_1,â€¦,V_{t-1})$$ï¼Œè€Œ$$t$$ä»¥åŠ$$t$$æ—¶é—´ä¹‹åçš„$$V$$æ˜¯æ²¡æœ‰æ•ˆæœçš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼šåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œç¬¬tä¸ªæ—¶é—´çš„queryåªçœ‹å‰è¾¹çš„key-valueå¯¹ï¼Œä½¿å¾—åšé¢„æµ‹çš„æ—¶å€™ä¸å®é™…æƒ…å†µæ˜¯å¯ä»¥ä¸€ä¸€å¯¹åº”ä¸Šçš„ã€‚ 1-5-2-2 Multi-Head AttentionæŠŠæ•´ä¸ªqueryã€key-valueæŠ•å½±åˆ°ä¸€ä¸ªä½ç»´$$h$$æ¬¡ï¼Œç„¶ååš$$h$$æ¬¡çš„æ³¨æ„åŠ›å‡½æ•°ï¼ŒæŠŠæ¯ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºå¹¶åœ¨ä¸€èµ·æŠ•å½±å›æ¥å°±ä¼šå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚ å°†key-valueæŠ•å½±åˆ°$$d_k=d_v=d_{model}(d_{output})/h$$è¿™ä¹ˆå¤§çš„ç»´åº¦ä¸Šã€‚ 1-5-2-3 Attentionåœ¨æ¨¡å‹ä¸­çš„åº”ç”¨ç¤ºæ„å›¾ä¸­3ä¸ªattentionä¸åŒçš„ä½œç”¨(æŠŠæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯èšåˆèµ·æ¥) 1ã€æ˜¯ä¸€ä¸ªè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¾“å…¥nä¸ªè¯ï¼Œæ¯ä¸ªè¯éƒ½æ˜¯é•¿ä¸ºdçš„å‘é‡ã€‚ç»è¿‡attentionçš„è¾“å‡ºçš„å°ºå¯¸æ˜¯å’Œè¾“å…¥ä¸€æ ·çš„ã€‚ 2ã€æ˜¯å¸¦Maskedçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚ä¼šå±è”½æ‰tæ—¶é—´ä»¥åŠtæ—¶é—´ä¹‹åçš„Qã€Kã€Vçš„è¿ç®—ã€‚ 3ã€Qæ˜¯é€šè¿‡Masked attentionçš„è¾“å‡ºï¼ŒK-Væ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºï¼Œè¿›è¡Œattentionè¿ç®—ã€‚ 1-5-3 Feed-Forwardpytorchä¸­ï¼Œè¾“å…¥æ˜¯3dæ—¶ï¼Œé»˜è®¤æ˜¯å¯¹æœ€åä¸€ä¸ªç»´åº¦(d)è¿›è¡Œè®¡ç®—ã€‚å…¬å¼å¦‚ä¸‹ï¼š ä½¿ç”¨ç›¸åŒçš„MLPå¯¹æ¯ä¸€ä¸ªç»è¿‡attentionçš„è¾“å‡ºè¿›è¡ŒæŠ•å½±å’ŒåæŠ•å½±ï¼Œä¸‹å›¾æ˜¯transformerå’ŒRNNçš„å¯¹æ¯”ï¼š 1-5-4 EmbeddingåµŒå…¥å±‚ã€‚å°†ä»»ä½•ä¸€ä¸ªè¯ç”¨é•¿ä¸ºdçš„ä¸€ä¸ªå‘é‡æ¥è¡¨ç¤ºã€‚ 1-5-5 Positional Encodingå¤„ç†æ—¶åºä¿¡æ¯ã€‚åœ¨è¾“å…¥é‡ŒåŠ å…¥æ—¶åºä¿¡æ¯ï¼Œå³å°†æ¯ä¸€ä¸ªè¯çš„ä½ç½®ä¿¡æ¯(æ•°å­—)è½¬æ¢æˆä¸€ä¸ªé•¿ä¸ºdçš„å‘é‡ç„¶åä¸åµŒå…¥å±‚çš„è¾“å‡ºåŠ èµ·æ¥ï¼Œä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥ã€‚æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºæ˜¯ä¸Qã€Kçš„ç›¸ä¼¼åº¦æœ‰å…³ï¼Œä¸ä½ç½®ä¿¡æ¯æ— å…³ï¼ŒåŠ å…¥ä½ç½®ä¿¡æ¯åå°±å¯ä»¥é¿å…å‡ºç°è¯­ä¹‰å› ä¸ºè¯çš„é¡ºåºè¢«æ‰“ä¹±è€Œå‘ç”Ÿå˜åŒ–ã€‚å…¬å¼å¦‚ä¸‹ï¼š å…¶ä¸­ï¼Œposæ˜¯positionï¼Œiæ˜¯dimensionã€‚512ä¸­å¶æ•°2iä½¿ç”¨sinï¼Œå¥‡æ•°2i+1ä½¿ç”¨cosã€‚ 1-6 å®éªŒTraining data and batchã€hardware and scheduleã€optimizerã€regularization(æ­£åˆ™åŒ–) 1-7 è®¨è®º2 ä»£ç å¤ç°2-1 ä½¿ç”¨å®ä¾‹ï¼š1234transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)src = torch.rand((10, 32, 512))tgt = torch.rand((20, 32, 512))out = transformer_model(src, tgt) 2-2 PyTorchæºç å®ç°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428import copyfrom typing import Optional, Anyimport torchfrom torch import Tensorfrom .. import functional as Ffrom .module import Modulefrom .activation import MultiheadAttentionfrom .container import ModuleListfrom ..init import xavier_uniform_from .dropout import Dropoutfrom .linear import Linearfrom .normalization import LayerNormclass Transformer(Module): r&quot;&quot;&quot;A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters. Args: d_model: the number of expected features in the encoder/decoder inputs (default=512). nhead: the number of heads in the multiheadattention models (default=8). num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6). num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). custom_encoder: custom encoder (default=None). custom_decoder: custom decoder (default=None). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False`` (seq, batch, feature). Examples:: &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12) &gt;&gt;&gt; src = torch.rand((10, 32, 512)) &gt;&gt;&gt; tgt = torch.rand((20, 32, 512)) &gt;&gt;&gt; out = transformer_model(src, tgt) Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model &quot;&quot;&quot; def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = &quot;relu&quot;, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None, layer_norm_eps: float = 1e-5, batch_first: bool = False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(Transformer, self).__init__() if custom_encoder is not None: self.encoder = custom_encoder else: encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, **factory_kwargs) encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm) if custom_decoder is not None: self.decoder = custom_decoder else: decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, **factory_kwargs) decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm) self._reset_parameters() self.d_model = d_model self.nhead = nhead self.batch_first = batch_first def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Take in and process masked source/target sequences. Args: src: the sequence to the encoder (required). tgt: the sequence to the decoder (required). src_mask: the additive mask for the src sequence (optional). tgt_mask: the additive mask for the tgt sequence (optional). memory_mask: the additive mask for the encoder output (optional). src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional). Shape: - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first. - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first. - src_mask: :math:`(S, S)`. - tgt_mask: :math:`(T, T)`. - memory_mask: :math:`(T, S)`. - src_key_padding_mask: :math:`(N, S)`. - tgt_key_padding_mask: :math:`(N, T)`. - memory_key_padding_mask: :math:`(N, S)`. Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first. Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number Examples: &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask) &quot;&quot;&quot; if not self.batch_first and src.size(1) != tgt.size(1): raise RuntimeError(&quot;the batch number of src and tgt must be equal&quot;) elif self.batch_first and src.size(0) != tgt.size(0): raise RuntimeError(&quot;the batch number of src and tgt must be equal&quot;) if src.size(2) != self.d_model or tgt.size(2) != self.d_model: raise RuntimeError(&quot;the feature number of src and tgt must be equal to d_model&quot;) memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask) output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask) return output def generate_square_subsequent_mask(self, sz: int) -&gt; Tensor: r&quot;&quot;&quot;Generate a square mask for the sequence. The masked positions are filled with float(&#x27;-inf&#x27;). Unmasked positions are filled with float(0.0). &quot;&quot;&quot; mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float(&#x27;-inf&#x27;)).masked_fill(mask == 1, float(0.0)) return mask def _reset_parameters(self): r&quot;&quot;&quot;Initiate parameters in the transformer model.&quot;&quot;&quot; for p in self.parameters(): if p.dim() &gt; 1: xavier_uniform_(p)class TransformerEncoder(Module): r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers Args: encoder_layer: an instance of the TransformerEncoderLayer() class (required). num_layers: the number of sub-encoder-layers in the encoder (required). norm: the layer normalization component (optional). Examples:: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = transformer_encoder(src) &quot;&quot;&quot; __constants__ = [&#x27;norm&#x27;] def __init__(self, encoder_layer, num_layers, norm=None): super(TransformerEncoder, self).__init__() self.layers = _get_clones(encoder_layer, num_layers) self.num_layers = num_layers self.norm = norm def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the input through the encoder layers in turn. Args: src: the sequence to the encoder (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; output = src for mod in self.layers: output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask) if self.norm is not None: output = self.norm(output) return outputclass TransformerDecoder(Module): r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers Args: decoder_layer: an instance of the TransformerDecoderLayer() class (required). num_layers: the number of sub-decoder-layers in the decoder (required). norm: the layer normalization component (optional). Examples:: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = transformer_decoder(tgt, memory) &quot;&quot;&quot; __constants__ = [&#x27;norm&#x27;] def __init__(self, decoder_layer, num_layers, norm=None): super(TransformerDecoder, self).__init__() self.layers = _get_clones(decoder_layer, num_layers) self.num_layers = num_layers self.norm = norm def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn. Args: tgt: the sequence to the decoder (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; output = tgt for mod in self.layers: output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask) if self.norm is not None: output = self.norm(output) return outputclass TransformerEncoderLayer(Module): r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args: d_model: the number of expected features in the input (required). nhead: the number of heads in the multiheadattention models (required). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of intermediate layer, relu or gelu (default=relu). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``. Examples:: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = encoder_layer(src) Alternatively, when ``batch_first`` is ``True``: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True) &gt;&gt;&gt; src = torch.rand(32, 10, 512) &gt;&gt;&gt; out = encoder_layer(src) &quot;&quot;&quot; __constants__ = [&#x27;batch_first&#x27;] def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&quot;relu&quot;, layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(TransformerEncoderLayer, self).__init__() self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # Implementation of Feedforward model self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.dropout = Dropout(dropout) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.dropout1 = Dropout(dropout) self.dropout2 = Dropout(dropout) self.activation = _get_activation_fn(activation) def __setstate__(self, state): if &#x27;activation&#x27; not in state: state[&#x27;activation&#x27;] = F.relu super(TransformerEncoderLayer, self).__setstate__(state) def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the input through the encoder layer. Args: src: the sequence to the encoder layer (required). src_mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0] src = src + self.dropout1(src2) src = self.norm1(src) src2 = self.linear2(self.dropout(self.activation(self.linear1(src)))) src = src + self.dropout2(src2) src = self.norm2(src) return srcclass TransformerDecoderLayer(Module): r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args: d_model: the number of expected features in the input (required). nhead: the number of heads in the multiheadattention models (required). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of intermediate layer, relu or gelu (default=relu). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``. Examples:: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory) Alternatively, when ``batch_first`` is ``True``: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True) &gt;&gt;&gt; memory = torch.rand(32, 10, 512) &gt;&gt;&gt; tgt = torch.rand(32, 20, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory) &quot;&quot;&quot; __constants__ = [&#x27;batch_first&#x27;] def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&quot;relu&quot;, layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(TransformerDecoderLayer, self).__init__() self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # Implementation of Feedforward model self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.dropout = Dropout(dropout) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.dropout1 = Dropout(dropout) self.dropout2 = Dropout(dropout) self.dropout3 = Dropout(dropout) self.activation = _get_activation_fn(activation) def __setstate__(self, state): if &#x27;activation&#x27; not in state: state[&#x27;activation&#x27;] = F.relu super(TransformerDecoderLayer, self).__setstate__(state) def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer. Args: tgt: the sequence to the decoder layer (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0] tgt = tgt + self.dropout1(tgt2) tgt = self.norm1(tgt) tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0] tgt = tgt + self.dropout2(tgt2) tgt = self.norm2(tgt) tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) tgt = tgt + self.dropout3(tgt2) tgt = self.norm3(tgt) return tgtdef _get_clones(module, N): return ModuleList([copy.deepcopy(module) for i in range(N)])def _get_activation_fn(activation): if activation == &quot;relu&quot;: return F.relu elif activation == &quot;gelu&quot;: return F.gelu raise RuntimeError(&quot;activation should be relu/gelu, not &#123;&#125;&quot;.format(activation)) 2-3 word embeddingã€positional encoding12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as F# å…³äºword embeddingï¼Œä»¥åºåˆ—å»ºæ¨¡ä¸ºä¾‹# è€ƒè™‘source sentenceå’Œtarget sentence# æ„å»ºåºåˆ—ï¼Œåºåˆ—çš„å­—ç¬¦ä»¥å…¶åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•çš„å½¢å¼è¡¨ç¤º# batch.size = 2# src_len = torch.randint(2, 5, (batch.size,))# tgt_len = torch.randint(2, 5, (batch.size,))# å•è¯è¡¨å¤§å°max_num_src_words = 8max_num_tgt_words = 8model_dim = 8# åºåˆ—ä¸ªæ•°å’Œé•¿åº¦src_len = torch.Tensor((2, 4)).to(torch.int32) # ä¸¤æ¡åºåˆ—ï¼Œé•¿åº¦åˆ†åˆ«ä¸º2ï¼Œ4tgt_len = torch.Tensor((4, 3)).to(torch.int32)# å•è¯ç´¢å¼•æ„æˆæºå¥å­å’Œç›®æ ‡å¥å­ï¼Œæ„å»ºbatchï¼Œå¹¶ä¸”åšäº†paddingï¼Œé»˜è®¤å€¼ä¸º0src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, \\ (L,)), (0, max(src_len)-L)), 0) for L in src_len]) # 1*4*2tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, \\ (L,)), (0, max(tgt_len)-L)), 0) for L in tgt_len])# æ„é€ embeddingsrc_embedding_table = nn.Embedding(max_num_src_words+1, model_dim) # 9*8tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)src_embedding = src_embedding_table(src_seq) # 4*8*2tgt_embedding = tgt_embedding_table(tgt_seq)# positionçš„æœ€å¤§é•¿åº¦max_position_len = 5# æ„é€ positional encodingpos_mat = torch.arange(max_position_len).reshape((-1, 1)) # 5*1i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape((1, -1))/model_dim)pe_encoding_table = torch.zeros(max_position_len, model_dim)pe_encoding_table[:, 0::2] = torch.sin(pos_mat / i_mat)pe_encoding_table[:, 1::2] = torch.cos(pos_mat / i_mat)src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) \\ for _ in src_len]).to(torch.int32) # 1*4*2tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) \\ for _ in tgt_len]) .to(torch.int32)pe_encoding = nn.Embedding(max_position_len, model_dim) # 5*8pe_encoding.weight = nn.Parameter(pe_encoding_table, requires_grad=False)src_pe_encoding = pe_encoding(src_pos) # 4*8*2tgt_pe_encoding = pe_encoding(tgt_pos) 2-4 encoder self_attention mask123456789101112131415161718import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as F# æ„é€ encoderçš„self-attention mask# maskçš„shapeï¼š[batch_size, max_src_len, max_src_len]ï¼Œå€¼ä¸º1æˆ–-inf(è´Ÿæ— ç©·)src_len = torch.Tensor((2, 4)).to(torch.int32)valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(src_len)-L)), 0) for L in src_len]), 2) # 2*4*1# è®¡ç®—ä¸¤ä¸ªä¸‰ç»´tensorçš„çŸ©é˜µä¹˜æ³•ï¼Œtorch.bmm(a,b),(b,h,w)Ã—(b,w,h)-&gt;(b,h,h)valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2)) # 2*4*4invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrixmask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)score = torch.randn(batch_size, max(src_len), max(src_len)) # 2*4*4masked_score = score.masked_fill(mask_encoder_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-5 intra-attention mask12345678910111213141516171819import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Fsrc_len = torch.Tensor((2, 4)).to(torch.int32)tgt_len = torch.Tensor((4, 3)).to(torch.int32)# Q*K^Tçš„shapeï¼š[batch_size, tgt_seq_len, src_seq_len]valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(src_len)-L)), 0) for L in src_len]), 2) # 2*4*1 valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(tgt_len)-L)), 0) for L in tgt_len]), 2) # 2*4*1 valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2)) # 2*4*4invalid_cross_pos_matrix = 1-valid_cross_pos_matrixmask_cross_self_attention = invalid_cross_pos_matrix.to(torch.bool)score = torch.randn(batch_size, max(tgt_len), max(src_len)) # 2*4*4masked_score = score.masked_fill(mask_cross_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-6 decoder self_attention mask123456789101112131415import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Ftgt_len = torch.Tensor((4, 3)).to(torch.int32)valid_decoder_tri_matrix = torch.cat([torch.sequeeze(F.pad(torch.tril(torch.ones((L, L))), \\ (0, max(tgt_len)-L), 0, max(tgt_len)-L)), 0) for L in tgt_len]) # 2*4*4invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrixmask_decoder_self_attention = invalid_decoder_tri_matrix.to(torch.bool)score = torch.randn(batch_size, max(tgt_len), max(tgt_len)) # 2*4*4masked_score = score.masked_fill(mask_decoder_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-7 scaled-dot self-attention123456789101112import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Fdef scaled_dot_product_attention(Q, K, V, attn_mask): # shape of Q, K, V:(batch_size*num_head, seq_len, model_dim/num_head) score = torch.bmm(Q, K.transpose(-2, -1))/torch.sqrt(model.dim) masked_score = score.masked_fill(attn_mask, -1e9) prob = F.softmax(masked_score, -1) context = torch.bmm(prob, V) return context 3 Transformeræ¨¡å‹æ€»ç»“","tags":["python","deep learning"],"categories":["å­¦ä¹ "]},{"title":"Hello World","path":"/2023/01/12/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"},{"path":"/about/index.html","content":"å—¨ï¼æˆ‘æ˜¯ gdp97 ğŸ‘‹ä¸€ä¸ªå¹³å¹³æ— å¥‡çš„äºº"}]
[{"title":"Transformer","path":"/2023/01/12/Transformer/","content":"1-1 摘要转录模型：用一个序列生成另一个序列。依赖使用encoder-decoder架构的复杂的RNN或CNN。在encoder和decoder之间使用attention机制。 前两句说当前的序列转录模型是什么样的，第三句说提出了一个新的简单的网络架构：Transformer，仅依赖attention机制，没有CNN和RNN。接下来讲，在一些机器翻译实验上结果比其他的模型效果好。 1-2 结论是第一个仅使用attention机制的转录模型，将之前所有的循环层全部换成了multi-headed self-attention。 将该模型使用在机器翻译之外的领域，如图片、音频、视频；或者使生成变得更少时序性(generation less sequential) 1-3 引言在时序模型和转录问题中(语言模型和机器翻译)，最常用的是RNN(包括LSTM、GRU)。其中有两个主要的模型——循环语言模型和encoder-decoder架构(当输出结构化信息比较多的时候使用)。 RNN的特点和缺点：一个序列的计算是从左往右一步一步地做。假设序列为句子，则输入是一个词一个词地接着，对第t个词输出一个隐藏状态ht，ht是由前一个隐藏状态和当前第t个词本身决定的。优点是可以将之前学到的历史信息由ht-1放到这个ht中。缺点是由于是时序，一步一步计算的过程，难以并行。对于长序列很不友好，内存限制了他的批处理。 attention与RNN结合使用，把encoder的东西有效传给decoder。 提出Transformer，不使用RNN，是纯attention机制，是可以并行运算的。在更短的时间内，做到比之前更好的结果。 1-4 相关工作(background)使用CNN作为基本构件块，替换RNN减少时序计算：但是CNN对于长序列模型难以建模，因为两个像素点很远的话，需要很多层卷积，才能将隔得远的两个像素融合起来。Transformer中的attention机制可以看到所有的像素，一层就可以将整个序列看到。卷积可以有多个输出通道，每个输出通道可以识别不一样的模式。提出了一个多头注意力机制(Multi-Head Attention) 自注意力机制(Self-Attention)的概念 memory-network的概念 是第一个只使用attention的模型 1-5 模型序列模型中比较好的是一个叫做编码器和解码器的架构。什么是编码器和解码器？编码器是指将一个输入序列$$(x_1,…,x_n)$$转换成对应的向量的表示$$z=(z_1,…,z_n)$$。也就是编码器将一些原始的数据转换成机器学习可以理解的一系列向量。解码器是将编码器的输出作为输入，然后生成一个(一个时间只有一个元素)序列$$(y_1,…,y_m)$$，其中n和m不一定相等。每生成一个$$y_i$$(输出)也会作为下一时刻的输入，来生成下一时刻的输出，这叫做自回归模型。 transformer是将一些自注意力机制和逐点方式(point-wise)，全连接层用编码器和解码器架构来堆在一起的。 1-5-1 编解码器编码器：总共使用了6个完全一样的层。每个层中有两个子层：多头自注意力(multi-head self-attention)、基于位置的前馈网络(positionwise feed-forward network)——多层感知机。每个子层都使用了残差连接，在残差连接的加法计算之后，跟着一个层规范化(layer normalization)。公式为$$LayerNorm(x+Sublayer(x))$$。为了简化残差连接，所以使所有子层的输出的维度变成512。 注：batch normalization和layer normalization的区别 注：蓝色线是batch normalization，黄色线是layer normalization。 当数据是二维时(batch, feature)，batch norm是在feature维度上进行归一化；layer norm是在batch维度上进行归一化(即每个样本自身进行归一化处理)。 当数据是三维时(batch, sequence, feature)，batch norm是在feature维度上进行归一化；layer norm是在batch维度上进行归一化(即每个样本自身进行归一化处理)。每个样本的序列长度不一样长，如果样本长度变化较大时甚至测试时出现更长的样本，batch norm不太适用进行归一。layer norm是对样本自身进行归一化，比较适合。 编码器：总共使用了6个完全一样的层。每个层中有三个子层：带掩码的多头注意力机制(注意力机制是可以看到全部的输入，带掩码是保证在t时间不会看到t时间以后的输入，从而保证训练和预测的时候行为是一致的)、多头注意力机制、多层感知机。 1-5-2 注意力层注意力函数是将一个query和一些key-value对映射成一个输出的函数，这里所有的query、keys、values、output都是一些向量。output是value的加权和，所以output的维度和value的维度是一样的。 对应每一个value的权重，是由value对应的key-query的相似度(compatibility function)计算来的。 1-5-2-1 Scaled Dot-Product AttentionScaled Dot-Product Attention，是一个比较简单的注意力机制。输入是由$$d_k$$长度的queries和keys和$$d_v$$长度的values组成的(与输出的长度相等)。对query和所有的key依次进行内积，然后除以$$\\sqrt{d_k}$$，然后再通过softmax函数就可以得到n个非负的而且加起来和等于一的一个权重，将权重作用到value上就可以得到相应的输出。具体如下：使用两次矩阵乘法 最常见的两种注意力机制：additive attention(加型注意力)-用于处理query和key不等长的情况、dot-product(multi-plicative) attention(点积的注意力机制)。当$$d_k$$较大时，query和key点积的结果更大的值，其softmax的结果更趋向于1，其他结果较小值的softmax的结果更趋向于0。这样就会导致梯度比较小，使模型认为已经接近收敛，LOSS下降会变慢，所以进行了除以$$\\sqrt{d_k}$$的操作。 Masked的作用：普通的注意力机制$$Q$$会和所有的$$K$$进行点乘运算，而这里的需要将$$t$$时间之后的$$K$$屏蔽掉。做法就是将$$t$$以及$$t$$时间之后的$$Q、K$$点乘运算的值替换成非常大的负数(-1e9)，再经过softmax运算得到的结果就会变成零，即$$t$$以及$$t$$时间之后的V所对应的权重都为零。因此，计算输出的时候只用到了$$(V_1,…,V_{t-1})$$，而$$t$$以及$$t$$时间之后的$$V$$是没有效果的。也就是说：在训练的时候，第t个时间的query只看前边的key-value对，使得做预测的时候与实际情况是可以一一对应上的。 1-5-2-2 Multi-Head Attention把整个query、key-value投影到一个低维$$h$$次，然后做$$h$$次的注意力函数，把每一个函数的输出并在一起投影回来就会得到最终的输出。 将key-value投影到$$d_k=d_v=d_{model}(d_{output})/h$$这么大的维度上。 1-5-2-3 Attention在模型中的应用示意图中3个attention不同的作用(把整个序列的信息聚合起来) 1、是一个自注意力机制。输入n个词，每个词都是长为d的向量。经过attention的输出的尺寸是和输入一样的。 2、是带Masked的自注意力机制。会屏蔽掉t时间以及t时间之后的Q、K、V的运算。 3、Q是通过Masked attention的输出，K-V是通过自注意力机制的输出，进行attention运算。 1-5-3 Feed-Forwardpytorch中，输入是3d时，默认是对最后一个维度(d)进行计算。公式如下： 使用相同的MLP对每一个经过attention的输出进行投影和反投影，下图是transformer和RNN的对比： 1-5-4 Embedding嵌入层。将任何一个词用长为d的一个向量来表示。 1-5-5 Positional Encoding处理时序信息。在输入里加入时序信息，即将每一个词的位置信息(数字)转换成一个长为d的向量然后与嵌入层的输出加起来，作为注意力机制的输入。注意力机制的输出是与Q、K的相似度有关，与位置信息无关，加入位置信息后就可以避免出现语义因为词的顺序被打乱而发生变化。公式如下： 其中，pos是position，i是dimension。512中偶数2i使用sin，奇数2i+1使用cos。 1-6 实验Training data and batch、hardware and schedule、optimizer、regularization(正则化) 1-7 讨论2 代码复现2-1 使用实例：1234transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)src = torch.rand((10, 32, 512))tgt = torch.rand((20, 32, 512))out = transformer_model(src, tgt) 2-2 PyTorch源码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428import copyfrom typing import Optional, Anyimport torchfrom torch import Tensorfrom .. import functional as Ffrom .module import Modulefrom .activation import MultiheadAttentionfrom .container import ModuleListfrom ..init import xavier_uniform_from .dropout import Dropoutfrom .linear import Linearfrom .normalization import LayerNormclass Transformer(Module): r&quot;&quot;&quot;A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters. Args: d_model: the number of expected features in the encoder/decoder inputs (default=512). nhead: the number of heads in the multiheadattention models (default=8). num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6). num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). custom_encoder: custom encoder (default=None). custom_decoder: custom decoder (default=None). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False`` (seq, batch, feature). Examples:: &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12) &gt;&gt;&gt; src = torch.rand((10, 32, 512)) &gt;&gt;&gt; tgt = torch.rand((20, 32, 512)) &gt;&gt;&gt; out = transformer_model(src, tgt) Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model &quot;&quot;&quot; def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = &quot;relu&quot;, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None, layer_norm_eps: float = 1e-5, batch_first: bool = False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(Transformer, self).__init__() if custom_encoder is not None: self.encoder = custom_encoder else: encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, **factory_kwargs) encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm) if custom_decoder is not None: self.decoder = custom_decoder else: decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, **factory_kwargs) decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm) self._reset_parameters() self.d_model = d_model self.nhead = nhead self.batch_first = batch_first def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Take in and process masked source/target sequences. Args: src: the sequence to the encoder (required). tgt: the sequence to the decoder (required). src_mask: the additive mask for the src sequence (optional). tgt_mask: the additive mask for the tgt sequence (optional). memory_mask: the additive mask for the encoder output (optional). src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional). Shape: - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first. - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first. - src_mask: :math:`(S, S)`. - tgt_mask: :math:`(T, T)`. - memory_mask: :math:`(T, S)`. - src_key_padding_mask: :math:`(N, S)`. - tgt_key_padding_mask: :math:`(N, T)`. - memory_key_padding_mask: :math:`(N, S)`. Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first. Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number Examples: &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask) &quot;&quot;&quot; if not self.batch_first and src.size(1) != tgt.size(1): raise RuntimeError(&quot;the batch number of src and tgt must be equal&quot;) elif self.batch_first and src.size(0) != tgt.size(0): raise RuntimeError(&quot;the batch number of src and tgt must be equal&quot;) if src.size(2) != self.d_model or tgt.size(2) != self.d_model: raise RuntimeError(&quot;the feature number of src and tgt must be equal to d_model&quot;) memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask) output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask) return output def generate_square_subsequent_mask(self, sz: int) -&gt; Tensor: r&quot;&quot;&quot;Generate a square mask for the sequence. The masked positions are filled with float(&#x27;-inf&#x27;). Unmasked positions are filled with float(0.0). &quot;&quot;&quot; mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float(&#x27;-inf&#x27;)).masked_fill(mask == 1, float(0.0)) return mask def _reset_parameters(self): r&quot;&quot;&quot;Initiate parameters in the transformer model.&quot;&quot;&quot; for p in self.parameters(): if p.dim() &gt; 1: xavier_uniform_(p)class TransformerEncoder(Module): r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers Args: encoder_layer: an instance of the TransformerEncoderLayer() class (required). num_layers: the number of sub-encoder-layers in the encoder (required). norm: the layer normalization component (optional). Examples:: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = transformer_encoder(src) &quot;&quot;&quot; __constants__ = [&#x27;norm&#x27;] def __init__(self, encoder_layer, num_layers, norm=None): super(TransformerEncoder, self).__init__() self.layers = _get_clones(encoder_layer, num_layers) self.num_layers = num_layers self.norm = norm def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the input through the encoder layers in turn. Args: src: the sequence to the encoder (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; output = src for mod in self.layers: output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask) if self.norm is not None: output = self.norm(output) return outputclass TransformerDecoder(Module): r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers Args: decoder_layer: an instance of the TransformerDecoderLayer() class (required). num_layers: the number of sub-decoder-layers in the decoder (required). norm: the layer normalization component (optional). Examples:: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = transformer_decoder(tgt, memory) &quot;&quot;&quot; __constants__ = [&#x27;norm&#x27;] def __init__(self, decoder_layer, num_layers, norm=None): super(TransformerDecoder, self).__init__() self.layers = _get_clones(decoder_layer, num_layers) self.num_layers = num_layers self.norm = norm def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn. Args: tgt: the sequence to the decoder (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; output = tgt for mod in self.layers: output = mod(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask) if self.norm is not None: output = self.norm(output) return outputclass TransformerEncoderLayer(Module): r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args: d_model: the number of expected features in the input (required). nhead: the number of heads in the multiheadattention models (required). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of intermediate layer, relu or gelu (default=relu). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``. Examples:: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = encoder_layer(src) Alternatively, when ``batch_first`` is ``True``: &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True) &gt;&gt;&gt; src = torch.rand(32, 10, 512) &gt;&gt;&gt; out = encoder_layer(src) &quot;&quot;&quot; __constants__ = [&#x27;batch_first&#x27;] def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&quot;relu&quot;, layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(TransformerEncoderLayer, self).__init__() self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # Implementation of Feedforward model self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.dropout = Dropout(dropout) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.dropout1 = Dropout(dropout) self.dropout2 = Dropout(dropout) self.activation = _get_activation_fn(activation) def __setstate__(self, state): if &#x27;activation&#x27; not in state: state[&#x27;activation&#x27;] = F.relu super(TransformerEncoderLayer, self).__setstate__(state) def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the input through the encoder layer. Args: src: the sequence to the encoder layer (required). src_mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0] src = src + self.dropout1(src2) src = self.norm1(src) src2 = self.linear2(self.dropout(self.activation(self.linear1(src)))) src = src + self.dropout2(src2) src = self.norm2(src) return srcclass TransformerDecoderLayer(Module): r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args: d_model: the number of expected features in the input (required). nhead: the number of heads in the multiheadattention models (required). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of intermediate layer, relu or gelu (default=relu). layer_norm_eps: the eps value in layer normalization components (default=1e-5). batch_first: If ``True``, then the input and output tensors are provided as (batch, seq, feature). Default: ``False``. Examples:: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory) Alternatively, when ``batch_first`` is ``True``: &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True) &gt;&gt;&gt; memory = torch.rand(32, 10, 512) &gt;&gt;&gt; tgt = torch.rand(32, 20, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory) &quot;&quot;&quot; __constants__ = [&#x27;batch_first&#x27;] def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&quot;relu&quot;, layer_norm_eps=1e-5, batch_first=False, device=None, dtype=None) -&gt; None: factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125; super(TransformerDecoderLayer, self).__init__() self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs) # Implementation of Feedforward model self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) self.dropout = Dropout(dropout) self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs) self.dropout1 = Dropout(dropout) self.dropout2 = Dropout(dropout) self.dropout3 = Dropout(dropout) self.activation = _get_activation_fn(activation) def __setstate__(self, state): if &#x27;activation&#x27; not in state: state[&#x27;activation&#x27;] = F.relu super(TransformerDecoderLayer, self).__setstate__(state) def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor: r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer. Args: tgt: the sequence to the decoder layer (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. &quot;&quot;&quot; tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0] tgt = tgt + self.dropout1(tgt2) tgt = self.norm1(tgt) tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0] tgt = tgt + self.dropout2(tgt2) tgt = self.norm2(tgt) tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) tgt = tgt + self.dropout3(tgt2) tgt = self.norm3(tgt) return tgtdef _get_clones(module, N): return ModuleList([copy.deepcopy(module) for i in range(N)])def _get_activation_fn(activation): if activation == &quot;relu&quot;: return F.relu elif activation == &quot;gelu&quot;: return F.gelu raise RuntimeError(&quot;activation should be relu/gelu, not &#123;&#125;&quot;.format(activation)) 2-3 word embedding、positional encoding12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as F# 关于word embedding，以序列建模为例# 考虑source sentence和target sentence# 构建序列，序列的字符以其在词表中的索引的形式表示# batch.size = 2# src_len = torch.randint(2, 5, (batch.size,))# tgt_len = torch.randint(2, 5, (batch.size,))# 单词表大小max_num_src_words = 8max_num_tgt_words = 8model_dim = 8# 序列个数和长度src_len = torch.Tensor((2, 4)).to(torch.int32) # 两条序列，长度分别为2，4tgt_len = torch.Tensor((4, 3)).to(torch.int32)# 单词索引构成源句子和目标句子，构建batch，并且做了padding，默认值为0src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, \\ (L,)), (0, max(src_len)-L)), 0) for L in src_len]) # 1*4*2tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, \\ (L,)), (0, max(tgt_len)-L)), 0) for L in tgt_len])# 构造embeddingsrc_embedding_table = nn.Embedding(max_num_src_words+1, model_dim) # 9*8tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)src_embedding = src_embedding_table(src_seq) # 4*8*2tgt_embedding = tgt_embedding_table(tgt_seq)# position的最大长度max_position_len = 5# 构造positional encodingpos_mat = torch.arange(max_position_len).reshape((-1, 1)) # 5*1i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape((1, -1))/model_dim)pe_encoding_table = torch.zeros(max_position_len, model_dim)pe_encoding_table[:, 0::2] = torch.sin(pos_mat / i_mat)pe_encoding_table[:, 1::2] = torch.cos(pos_mat / i_mat)src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) \\ for _ in src_len]).to(torch.int32) # 1*4*2tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) \\ for _ in tgt_len]) .to(torch.int32)pe_encoding = nn.Embedding(max_position_len, model_dim) # 5*8pe_encoding.weight = nn.Parameter(pe_encoding_table, requires_grad=False)src_pe_encoding = pe_encoding(src_pos) # 4*8*2tgt_pe_encoding = pe_encoding(tgt_pos) 2-4 encoder self_attention mask123456789101112131415161718import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as F# 构造encoder的self-attention mask# mask的shape：[batch_size, max_src_len, max_src_len]，值为1或-inf(负无穷)src_len = torch.Tensor((2, 4)).to(torch.int32)valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(src_len)-L)), 0) for L in src_len]), 2) # 2*4*1# 计算两个三维tensor的矩阵乘法，torch.bmm(a,b),(b,h,w)×(b,w,h)-&gt;(b,h,h)valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2)) # 2*4*4invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrixmask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)score = torch.randn(batch_size, max(src_len), max(src_len)) # 2*4*4masked_score = score.masked_fill(mask_encoder_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-5 intra-attention mask12345678910111213141516171819import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Fsrc_len = torch.Tensor((2, 4)).to(torch.int32)tgt_len = torch.Tensor((4, 3)).to(torch.int32)# Q*K^T的shape：[batch_size, tgt_seq_len, src_seq_len]valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(src_len)-L)), 0) for L in src_len]), 2) # 2*4*1 valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \\ (0, max(tgt_len)-L)), 0) for L in tgt_len]), 2) # 2*4*1 valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2)) # 2*4*4invalid_cross_pos_matrix = 1-valid_cross_pos_matrixmask_cross_self_attention = invalid_cross_pos_matrix.to(torch.bool)score = torch.randn(batch_size, max(tgt_len), max(src_len)) # 2*4*4masked_score = score.masked_fill(mask_cross_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-6 decoder self_attention mask123456789101112131415import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Ftgt_len = torch.Tensor((4, 3)).to(torch.int32)valid_decoder_tri_matrix = torch.cat([torch.sequeeze(F.pad(torch.tril(torch.ones((L, L))), \\ (0, max(tgt_len)-L), 0, max(tgt_len)-L)), 0) for L in tgt_len]) # 2*4*4invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrixmask_decoder_self_attention = invalid_decoder_tri_matrix.to(torch.bool)score = torch.randn(batch_size, max(tgt_len), max(tgt_len)) # 2*4*4masked_score = score.masked_fill(mask_decoder_self_attention, -np.inf)prob = F.softmax(masked_score, -1) 2-7 scaled-dot self-attention123456789101112import torchimport numpy as npimport torch.nn as nnimport torch.nn.functional as Fdef scaled_dot_product_attention(Q, K, V, attn_mask): # shape of Q, K, V:(batch_size*num_head, seq_len, model_dim/num_head) score = torch.bmm(Q, K.transpose(-2, -1))/torch.sqrt(model.dim) masked_score = score.masked_fill(attn_mask, -1e9) prob = F.softmax(masked_score, -1) context = torch.bmm(prob, V) return context 3 Transformer模型总结","tags":["python，deep learning"],"categories":["学习"]},{"title":"Hello World","path":"/2023/01/12/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]
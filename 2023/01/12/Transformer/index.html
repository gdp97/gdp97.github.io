<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.4.2">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>Transformer - gdp97 的博客</title>

  
    <meta name="description" content="1-1 摘要转录模型：用一个序列生成另一个序列。依赖使用encoder-decoder架构的复杂的RNN或CNN。在encoder和decoder之间使用attention机制。 前两句说当前的序列转录模型是什么样的，第三句说提出了一个新的简单的网络架构：Transformer，仅依赖attention机制，没有CNN和RNN。接下来讲，在一些机器翻译实验上结果比其他的模型效果好。 1-2 结论是">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://example.com/2023/01/12/Transformer/index.html">
<meta property="og:site_name" content="gdp97 的博客">
<meta property="og:description" content="1-1 摘要转录模型：用一个序列生成另一个序列。依赖使用encoder-decoder架构的复杂的RNN或CNN。在encoder和decoder之间使用attention机制。 前两句说当前的序列转录模型是什么样的，第三句说提出了一个新的简单的网络架构：Transformer，仅依赖attention机制，没有CNN和RNN。接下来讲，在一些机器翻译实验上结果比其他的模型效果好。 1-2 结论是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/gdp97/mypicture/raw/master/202301121526508.png">
<meta property="og:image" content="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=YTk0NTRmZTFiNTZiOWEwM2Q3Mjg1OWQyNTllMWZiOTBfaWJLdDZ0QjVXb1NCbzZmaERwOHJvVGpjWlIwNUkxREZfVG9rZW46Ym94Y250S2tDNldXYm5uNkd3eVVGQ1BXbVNlXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA">
<meta property="og:image" content="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDkxOTdhZTkzNWE5NjFmM2Y4ZmZhZDVjZjY1NGEwNmNfeEpDMW9OWFVpMFNHNGVTUXBjWXB4bUYyRGRzc1N6R0tfVG9rZW46Ym94Y25sTFlUODdOUHVBWGRadXYzT1ptRHZlXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA">
<meta property="og:image" content="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2RmZDEyMzg1OTFiODg4YTE0NDIyM2M4MjY0M2JjYjJfVVRDSHlqaUhuNDFlZVlCUXg4bERRdUR5bFRUY0RqZTlfVG9rZW46Ym94Y25lOURIU0VlU1VITTNEYWp5WGFIQ05oXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA">
<meta property="og:image" content="https://gitee.com/gdp97/mypicture/raw/master/202301121526039.png">
<meta property="og:image" content="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=NGU2NzFmNTYxYjhiMzUwZDczNGFkZmZkNmNlY2Y4N2VfR00yTFZpZGFuelMzWVBHT1I4dldMdHd4amJ1VjF3WHpfVG9rZW46Ym94Y254YkN4YkNsbjhzZUt0V0lwWHBTVVdiXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA">
<meta property="og:image" content="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=NjQxODg3ZmJmMzNmODU5ZjQ0ZmRmZmY0YWRjYzEzMTVfcXZXVWFVNUNQSXVOQ2p6YkMwM0NjalgzOWJ6dFJoRGdfVG9rZW46Ym94Y251YnhaYjZqdXp0TUlNcWpudEVMREo0XzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA">
<meta property="article:published_time" content="2023-01-12T07:23:54.000Z">
<meta property="article:modified_time" content="2023-01-12T07:30:03.963Z">
<meta property="article:author" content="gdp97">
<meta property="article:tag" content="python，deep learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/gdp97/mypicture/raw/master/202301121526508.png">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="title" href="/"><div class="main" ff="title">gdp97 的博客</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">文章</a><a class="nav-item" href="/wiki/">项目</a><a class="nav-item" href="/about/">关于</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">Transformer</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E6%91%98%E8%A6%81"><span class="toc-text">1-1 摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E7%BB%93%E8%AE%BA"><span class="toc-text">1-2 结论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E5%BC%95%E8%A8%80"><span class="toc-text">1-3 引言</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-background"><span class="toc-text">1-4 相关工作(background)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E6%A8%A1%E5%9E%8B"><span class="toc-text">1-5 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-1-%E7%BC%96%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-text">1-5-1 编解码器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82"><span class="toc-text">1-5-2 注意力层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-3-Feed-Forward"><span class="toc-text">1-5-3 Feed-Forward</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-4-Embedding"><span class="toc-text">1-5-4 Embedding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-5-Positional-Encoding"><span class="toc-text">1-5-5 Positional Encoding</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-%E5%AE%9E%E9%AA%8C"><span class="toc-text">1-6 实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-%E8%AE%A8%E8%AE%BA"><span class="toc-text">1-7 讨论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="toc-text">2 代码复现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%EF%BC%9A"><span class="toc-text">2-1 使用实例：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-PyTorch%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2-2 PyTorch源码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-word-embedding%E3%80%81positional-encoding"><span class="toc-text">2-3 word embedding、positional encoding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-encoder-self-attention-mask"><span class="toc-text">2-4 encoder self_attention mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-intra-attention-mask"><span class="toc-text">2-5 intra-attention mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-decoder-self-attention-mask"><span class="toc-text">2-6 decoder self_attention mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-scaled-dot-self-attention"><span class="toc-text">2-7 scaled-dot self-attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Transformer%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-text">3 Transformer模型总结</span></a></div></div></widget>




</div>


    </aside>
    <div class='l_main'>
      

      


<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></div><div id="post-meta">发布于&nbsp;<time datetime="2023-01-12T07:23:54.000Z">2023-01-12</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>Transformer</span></h1>
<h4 id="1-1-摘要"><a href="#1-1-摘要" class="headerlink" title="1-1 摘要"></a>1-1 摘要</h4><p>转录模型：用一个序列生成另一个序列。依赖使用encoder-decoder架构的复杂的RNN或CNN。在encoder和decoder之间使用attention机制。</p>
<p>前两句说当前的序列转录模型是什么样的，第三句说提出了一个新的简单的网络架构：Transformer，仅依赖attention机制，没有CNN和RNN。接下来讲，在一些机器翻译实验上结果比其他的模型效果好。</p>
<h4 id="1-2-结论"><a href="#1-2-结论" class="headerlink" title="1-2 结论"></a>1-2 结论</h4><p>是第一个仅使用attention机制的转录模型，将之前所有的循环层全部换成了multi-headed self-attention。</p>
<p>将该模型使用在机器翻译之外的领域，如图片、音频、视频；或者使生成变得更少时序性(generation less sequential)</p>
<h4 id="1-3-引言"><a href="#1-3-引言" class="headerlink" title="1-3 引言"></a>1-3 引言</h4><p>在时序模型和转录问题中(语言模型和机器翻译)，最常用的是RNN(包括LSTM、GRU)。其中有两个主要的模型——循环语言模型和encoder-decoder架构(当输出结构化信息比较多的时候使用)。</p>
<p>RNN的特点和缺点：一个序列的计算是从左往右一步一步地做。假设序列为句子，则输入是一个词一个词地接着，对第t个词输出一个隐藏状态ht，ht是由前一个隐藏状态和当前第t个词本身决定的。优点是可以将之前学到的历史信息由ht-1放到这个ht中。缺点是由于是时序，一步一步计算的过程，难以并行。对于长序列很不友好，内存限制了他的批处理。</p>
<p>attention与RNN结合使用，把encoder的东西有效传给decoder。</p>
<p>提出Transformer，不使用RNN，是纯attention机制，是可以并行运算的。在更短的时间内，做到比之前更好的结果。</p>
<h4 id="1-4-相关工作-background"><a href="#1-4-相关工作-background" class="headerlink" title="1-4 相关工作(background)"></a>1-4 相关工作(background)</h4><p>使用CNN作为基本构件块，替换RNN减少时序计算：但是CNN对于长序列模型难以建模，因为两个像素点很远的话，需要很多层卷积，才能将隔得远的两个像素融合起来。Transformer中的attention机制可以看到所有的像素，一层就可以将整个序列看到。卷积可以有多个输出通道，每个输出通道可以识别不一样的模式。提出了一个多头注意力机制(Multi-Head Attention)</p>
<p>自注意力机制(Self-Attention)的概念</p>
<p>memory-network的概念</p>
<p>是第一个只使用attention的模型</p>
<h4 id="1-5-模型"><a href="#1-5-模型" class="headerlink" title="1-5 模型"></a>1-5 模型</h4><p>序列模型中比较好的是一个叫做编码器和解码器的架构。什么是编码器和解码器？编码器是指将一个输入序列$$(x_1,…,x_n)$$转换成<strong>对应的向量</strong>的表示$$z=(z_1,…,z_n)$$。也就是编码器将一些原始的数据转换成机器学习可以理解的一系列向量。解码器是将编码器的输出作为输入，然后生成一个(一个时间只有一个元素)序列$$(y_1,…,y_m)$$，其中n和m不一定相等。每生成一个$$y_i$$(输出)也会作为下一时刻的输入，来生成下一时刻的输出，这叫做自回归模型。</p>
<p>transformer是将一些自注意力机制和逐点方式(point-wise)，全连接层用编码器和解码器架构来堆在一起的。</p>
<h5 id="1-5-1-编解码器"><a href="#1-5-1-编解码器" class="headerlink" title="1-5-1 编解码器"></a>1-5-1 编解码器</h5><p><strong>编码器：</strong>总共使用了6个完全一样的层。每个层中有两个子层：多头自注意力(multi-head self-attention)、基于位置的前馈网络(positionwise feed-forward network)——多层感知机。每个子层都使用了残差连接，在残差连接的加法计算之后，跟着一个层规范化(layer normalization)。公式为$$LayerNorm(x+Sublayer(x))$$。为了简化残差连接，所以使所有子层的输出的维度变成512。</p>
<p>注：batch normalization和layer normalization的区别</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gitee.com/gdp97/mypicture/raw/master/202301121526508.png" alt="img" style="zoom:50%;" />

<p><strong>注：</strong>蓝色线是batch normalization，黄色线是layer normalization。</p>
<p>当数据是二维时(batch, feature)，batch norm是在feature维度上进行归一化；layer norm是在batch维度上进行归一化(即每个样本自身进行归一化处理)。</p>
<p>当数据是三维时(batch, sequence, feature)，batch norm是在feature维度上进行归一化；layer norm是在batch维度上进行归一化(即每个样本自身进行归一化处理)。每个样本的序列长度不一样长，如果样本长度变化较大时甚至测试时出现更长的样本，batch norm不太适用进行归一。layer norm是对样本自身进行归一化，比较适合。</p>
<p><strong>编码器：</strong>总共使用了6个完全一样的层。每个层中有三个子层：带掩码的多头注意力机制(注意力机制是可以看到全部的输入，带掩码是保证在t时间不会看到t时间以后的输入，从而保证训练和预测的时候行为是一致的)、多头注意力机制、多层感知机。</p>
<h5 id="1-5-2-注意力层"><a href="#1-5-2-注意力层" class="headerlink" title="1-5-2 注意力层"></a>1-5-2 注意力层</h5><p>注意力函数是将一个query和一些key-value对映射成一个输出的函数，这里所有的query、keys、values、output都是一些向量。output是value的加权和，所以output的维度和value的维度是一样的。</p>
<p>对应每一个value的权重，是由value对应的key-query的相似度(compatibility function)计算来的。</p>
<h6 id="1-5-2-1-Scaled-Dot-Product-Attention"><a href="#1-5-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="1-5-2-1 Scaled Dot-Product Attention"></a>1-5-2-1 Scaled Dot-Product Attention</h6><p>Scaled Dot-Product Attention，是一个比较简单的注意力机制。输入是由$$d_k$$长度的queries和keys和$$d_v$$长度的values组成的(与输出的长度相等)。对query和所有的key依次进行内积，然后除以$$\sqrt{d_k}$$，然后再通过softmax函数就可以得到n个非负的而且加起来和等于一的一个权重，将权重作用到value上就可以得到相应的输出。具体如下：<strong>使用两次矩阵乘法</strong></p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=YTk0NTRmZTFiNTZiOWEwM2Q3Mjg1OWQyNTllMWZiOTBfaWJLdDZ0QjVXb1NCbzZmaERwOHJvVGpjWlIwNUkxREZfVG9rZW46Ym94Y250S2tDNldXYm5uNkd3eVVGQ1BXbVNlXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA" alt="img" style="zoom: 50%;" />

<p>最常见的两种注意力机制：additive attention(加型注意力)-用于处理query和key不等长的情况、dot-product(multi-plicative) attention(点积的注意力机制)。当$$d_k$$较大时，query和key点积的结果更大的值，其softmax的结果更趋向于1，其他结果较小值的softmax的结果更趋向于0。这样就会导致梯度比较小，使模型认为已经接近收敛，LOSS下降会变慢，所以进行了除以$$\sqrt{d_k}$$的操作。</p>
<p><strong>Masked的作用</strong>：普通的注意力机制$$Q$$会和所有的$$K$$进行点乘运算，而这里的需要将$$t$$时间之后的$$K$$屏蔽掉。做法就是将$$t$$以及$$t$$时间之后的$$Q、K$$点乘运算的值替换成非常大的负数(-1e9)，再经过softmax运算得到的结果就会变成零，即$$t$$以及$$t$$时间之后的V所对应的权重都为零。因此，计算输出的时候只用到了$$(V_1,…,V_{t-1})$$，而$$t$$以及$$t$$时间之后的$$V$$是没有效果的。也就是说：<strong>在训练的时候，第t个时间的query只看前边的key-value对，使得做预测的时候与实际情况是可以一一对应上的。</strong></p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=ZDkxOTdhZTkzNWE5NjFmM2Y4ZmZhZDVjZjY1NGEwNmNfeEpDMW9OWFVpMFNHNGVTUXBjWXB4bUYyRGRzc1N6R0tfVG9rZW46Ym94Y25sTFlUODdOUHVBWGRadXYzT1ptRHZlXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA" alt="img" style="zoom:50%;" />

<h6 id="1-5-2-2-Multi-Head-Attention"><a href="#1-5-2-2-Multi-Head-Attention" class="headerlink" title="1-5-2-2 Multi-Head Attention"></a>1-5-2-2 Multi-Head Attention</h6><p>把整个query、key-value投影到一个低维$$h$$次，然后做$$h$$次的注意力函数，把每一个函数的输出并在一起投影回来就会得到最终的输出。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2RmZDEyMzg1OTFiODg4YTE0NDIyM2M4MjY0M2JjYjJfVVRDSHlqaUhuNDFlZVlCUXg4bERRdUR5bFRUY0RqZTlfVG9rZW46Ym94Y25lOURIU0VlU1VITTNEYWp5WGFIQ05oXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA" alt="img"></p>
<p>将key-value投影到$$d_k=d_v=d_{model}(d_{output})/h$$这么大的维度上。</p>
<h6 id="1-5-2-3-Attention在模型中的应用"><a href="#1-5-2-3-Attention在模型中的应用" class="headerlink" title="1-5-2-3 Attention在模型中的应用"></a>1-5-2-3 Attention在模型中的应用</h6><p>示意图中3个attention不同的作用(把整个序列的信息聚合起来)</p>
<p>1、是一个<strong>自注意力机制。</strong>输入n个词，每个词都是长为d的向量。经过attention的输出的尺寸是和输入一样的。</p>
<p>2、是带Masked的自注意力机制。会屏蔽掉<code>t</code>时间以及<code>t</code>时间之后的Q、K、V的运算。</p>
<p>3、Q是通过Masked attention的输出，K-V是通过<strong>自注意力机制</strong>的输出，进行attention运算。</p>
<h5 id="1-5-3-Feed-Forward"><a href="#1-5-3-Feed-Forward" class="headerlink" title="1-5-3 Feed-Forward"></a>1-5-3 Feed-Forward</h5><p>pytorch中，输入是3d时，默认是对最后一个维度(<code>d</code>)进行计算。公式如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gitee.com/gdp97/mypicture/raw/master/202301121526039.png" alt="img"></p>
<p>使用相同的MLP对每一个经过attention的输出进行投影和反投影，下图是transformer和RNN的对比：</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=NGU2NzFmNTYxYjhiMzUwZDczNGFkZmZkNmNlY2Y4N2VfR00yTFZpZGFuelMzWVBHT1I4dldMdHd4amJ1VjF3WHpfVG9rZW46Ym94Y254YkN4YkNsbjhzZUt0V0lwWHBTVVdiXzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA" alt="img" style="zoom:33%;" />

<h5 id="1-5-4-Embedding"><a href="#1-5-4-Embedding" class="headerlink" title="1-5-4 Embedding"></a>1-5-4 Embedding</h5><p>嵌入层。将任何一个词用长为<code>d</code>的一个向量来表示。</p>
<h5 id="1-5-5-Positional-Encoding"><a href="#1-5-5-Positional-Encoding" class="headerlink" title="1-5-5 Positional Encoding"></a>1-5-5 Positional Encoding</h5><p>处理时序信息。在输入里加入时序信息，即将每一个词的位置信息(数字)转换成一个长为<code>d</code>的向量然后与嵌入层的输出加起来，作为注意力机制的输入。注意力机制的输出是与Q、K的相似度有关，与位置信息无关，加入位置信息后就可以避免出现语义因为词的顺序被打乱而发生变化。公式如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://dphzb0p8kb.feishu.cn/space/api/box/stream/download/asynccode/?code=NjQxODg3ZmJmMzNmODU5ZjQ0ZmRmZmY0YWRjYzEzMTVfcXZXVWFVNUNQSXVOQ2p6YkMwM0NjalgzOWJ6dFJoRGdfVG9rZW46Ym94Y251YnhaYjZqdXp0TUlNcWpudEVMREo0XzE2NzM1MDg0MDY6MTY3MzUxMjAwNl9WNA" alt="img"></p>
<p>其中，<code>pos</code>是<code>position</code>，<code>i</code>是<code>dimension</code>。512中偶数<code>2i</code>使用<code>sin</code>，奇数<code>2i+1</code>使用<code>cos</code>。</p>
<h4 id="1-6-实验"><a href="#1-6-实验" class="headerlink" title="1-6 实验"></a>1-6 实验</h4><p>Training data and batch、hardware and schedule、optimizer、regularization(正则化)</p>
<h4 id="1-7-讨论"><a href="#1-7-讨论" class="headerlink" title="1-7 讨论"></a>1-7 讨论</h4><h3 id="2-代码复现"><a href="#2-代码复现" class="headerlink" title="2 代码复现"></a>2 代码复现</h3><h4 id="2-1-使用实例："><a href="#2-1-使用实例：" class="headerlink" title="2-1 使用实例："></a>2-1 使用实例：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transformer_model = nn.Transformer(nhead=<span class="number">16</span>, num_encoder_layers=<span class="number">12</span>)</span><br><span class="line">src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">out = transformer_model(src, tgt)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-PyTorch源码实现"><a href="#2-2-PyTorch源码实现" class="headerlink" title="2-2 PyTorch源码实现"></a>2-2 PyTorch源码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> .. <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> .module <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">from</span> .activation <span class="keyword">import</span> MultiheadAttention</span><br><span class="line"><span class="keyword">from</span> .container <span class="keyword">import</span> ModuleList</span><br><span class="line"><span class="keyword">from</span> ..init <span class="keyword">import</span> xavier_uniform_</span><br><span class="line"><span class="keyword">from</span> .dropout <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> .linear <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> .normalization <span class="keyword">import</span> LayerNorm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;A transformer model. User is able to modify the attributes as needed. The architecture</span></span><br><span class="line"><span class="string">    is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer,</span></span><br><span class="line"><span class="string">    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and</span></span><br><span class="line"><span class="string">    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information</span></span><br><span class="line"><span class="string">    Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)</span></span><br><span class="line"><span class="string">    model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the encoder/decoder inputs (default=512).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (default=8).</span></span><br><span class="line"><span class="string">        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).</span></span><br><span class="line"><span class="string">        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).</span></span><br><span class="line"><span class="string">        custom_encoder: custom encoder (default=None).</span></span><br><span class="line"><span class="string">        custom_decoder: custom decoder (default=None).</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: A full example to apply nn.Transformer module for the word language model is available in</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/examples/tree/master/word_language_model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span> = <span class="number">512</span>, nhead: <span class="built_in">int</span> = <span class="number">8</span>, num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>,</span></span><br><span class="line"><span class="params">                 num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 activation: <span class="built_in">str</span> = <span class="string">&quot;relu&quot;</span>, custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.encoder = custom_encoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first,</span><br><span class="line">                                                    **factory_kwargs)</span><br><span class="line">            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.decoder = custom_decoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first,</span><br><span class="line">                                                    **factory_kwargs)</span><br><span class="line">            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.nhead = nhead</span><br><span class="line"></span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, tgt: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Take in and process masked source/target sequences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            src_mask: the additive mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_mask: the additive mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the additive mask for the encoder output (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: :math:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: :math:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: :math:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: :math:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked</span></span><br><span class="line"><span class="string">            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend</span></span><br><span class="line"><span class="string">            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``</span></span><br><span class="line"><span class="string">            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor</span></span><br><span class="line"><span class="string">            is provided, it will be added to the attention weight.</span></span><br><span class="line"><span class="string">            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by</span></span><br><span class="line"><span class="string">            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero</span></span><br><span class="line"><span class="string">            positions will be unchanged. If a BoolTensor is provided, the positions with the</span></span><br><span class="line"><span class="string">            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note: Due to the multi-head attention architecture in the transformer model,</span></span><br><span class="line"><span class="string">            the output sequence length of a transformer is same as the input sequence</span></span><br><span class="line"><span class="string">            (i.e. target) length of the decode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            where S is the source sequence length, T is the target sequence length, N is the</span></span><br><span class="line"><span class="string">            batch size, E is the feature number</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Examples:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.batch_first <span class="keyword">and</span> src.size(<span class="number">1</span>) != tgt.size(<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the batch number of src and tgt must be equal&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.batch_first <span class="keyword">and</span> src.size(<span class="number">0</span>) != tgt.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the batch number of src and tgt must be equal&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> src.size(<span class="number">2</span>) != self.d_model <span class="keyword">or</span> tgt.size(<span class="number">2</span>) != self.d_model:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the feature number of src and tgt must be equal to d_model&quot;</span>)</span><br><span class="line"></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_square_subsequent_mask</span>(<span class="params">self, sz: <span class="built_in">int</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Generate a square mask for the sequence. The masked positions are filled with float(&#x27;-inf&#x27;).</span></span><br><span class="line"><span class="string">            Unmasked positions are filled with float(0.0).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.<span class="built_in">float</span>().masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class="number">1</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Initiate parameters in the transformer model.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard encoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of intermediate layer, relu or gelu (default=relu).</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=<span class="string">&quot;relu&quot;</span>,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src: Tensor, src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder layer (required).</span></span><br><span class="line"><span class="string">            src_mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        src2 = self.self_attn(src, src, src, attn_mask=src_mask,</span><br><span class="line">                              key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src = self.norm1(src)</span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        src = self.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of intermediate layer, relu or gelu (default=relu).</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=<span class="string">&quot;relu&quot;</span>,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.activation = _get_activation_fn(activation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">        tgt = self.norm1(tgt)</span><br><span class="line">        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">        tgt = self.norm2(tgt)</span><br><span class="line">        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))</span><br><span class="line">        tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">        tgt = self.norm3(tgt)</span><br><span class="line">        <span class="keyword">return</span> tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_activation_fn</span>(<span class="params">activation</span>):</span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.relu</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;gelu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.gelu</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;activation should be relu/gelu, not &#123;&#125;&quot;</span>.<span class="built_in">format</span>(activation))</span><br></pre></td></tr></table></figure>

<h4 id="2-3-word-embedding、positional-encoding"><a href="#2-3-word-embedding、positional-encoding" class="headerlink" title="2-3 word embedding、positional encoding"></a>2-3 word embedding、positional encoding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关于word embedding，以序列建模为例</span></span><br><span class="line"><span class="comment"># 考虑source sentence和target sentence</span></span><br><span class="line"><span class="comment"># 构建序列，序列的字符以其在词表中的索引的形式表示</span></span><br><span class="line"><span class="comment"># batch.size = 2</span></span><br><span class="line"><span class="comment"># src_len = torch.randint(2, 5, (batch.size,))</span></span><br><span class="line"><span class="comment"># tgt_len = torch.randint(2, 5, (batch.size,))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单词表大小</span></span><br><span class="line">max_num_src_words = <span class="number">8</span></span><br><span class="line">max_num_tgt_words = <span class="number">8</span></span><br><span class="line">model_dim = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列个数和长度</span></span><br><span class="line">src_len = torch.Tensor((<span class="number">2</span>, <span class="number">4</span>)).to(torch.int32)  <span class="comment"># 两条序列，长度分别为2，4</span></span><br><span class="line">tgt_len = torch.Tensor((<span class="number">4</span>, <span class="number">3</span>)).to(torch.int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单词索引构成源句子和目标句子，构建batch，并且做了padding，默认值为0</span></span><br><span class="line">src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>, max_num_src_words, \</span><br><span class="line">                    (L,)), (<span class="number">0</span>, <span class="built_in">max</span>(src_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len])  <span class="comment"># 1*4*2</span></span><br><span class="line">tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(<span class="number">1</span>, max_num_tgt_words, \</span><br><span class="line">                    (L,)), (<span class="number">0</span>, <span class="built_in">max</span>(tgt_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造embedding</span></span><br><span class="line">src_embedding_table = nn.Embedding(max_num_src_words+<span class="number">1</span>, model_dim)  <span class="comment"># 9*8</span></span><br><span class="line">tgt_embedding_table = nn.Embedding(max_num_tgt_words+<span class="number">1</span>, model_dim)</span><br><span class="line">src_embedding = src_embedding_table(src_seq)  <span class="comment"># 4*8*2</span></span><br><span class="line">tgt_embedding = tgt_embedding_table(tgt_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># position的最大长度</span></span><br><span class="line">max_position_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造positional encoding</span></span><br><span class="line">pos_mat = torch.arange(max_position_len).reshape((-<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 5*1</span></span><br><span class="line">i_mat = torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, model_dim, <span class="number">2</span>).reshape((<span class="number">1</span>, -<span class="number">1</span>))/model_dim)</span><br><span class="line">pe_encoding_table = torch.zeros(max_position_len, model_dim)</span><br><span class="line">pe_encoding_table[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos_mat / i_mat)</span><br><span class="line">pe_encoding_table[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos_mat / i_mat)</span><br><span class="line"></span><br><span class="line">src_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(src_len)), <span class="number">0</span>) \</span><br><span class="line">                    <span class="keyword">for</span> _ <span class="keyword">in</span> src_len]).to(torch.int32)  <span class="comment"># 1*4*2</span></span><br><span class="line">tgt_pos = torch.cat([torch.unsqueeze(torch.arange(<span class="built_in">max</span>(tgt_len)), <span class="number">0</span>) \</span><br><span class="line">                    <span class="keyword">for</span> _ <span class="keyword">in</span> tgt_len]) .to(torch.int32)</span><br><span class="line"></span><br><span class="line">pe_encoding = nn.Embedding(max_position_len, model_dim)  <span class="comment"># 5*8</span></span><br><span class="line">pe_encoding.weight = nn.Parameter(pe_encoding_table, requires_grad=<span class="literal">False</span>)</span><br><span class="line">src_pe_encoding = pe_encoding(src_pos)  <span class="comment"># 4*8*2</span></span><br><span class="line">tgt_pe_encoding = pe_encoding(tgt_pos)</span><br></pre></td></tr></table></figure>

<h4 id="2-4-encoder-self-attention-mask"><a href="#2-4-encoder-self-attention-mask" class="headerlink" title="2-4 encoder self_attention mask"></a>2-4 encoder self_attention mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造encoder的self-attention mask</span></span><br><span class="line"><span class="comment"># mask的shape：[batch_size, max_src_len, max_src_len]，值为1或-inf(负无穷)</span></span><br><span class="line">src_len = torch.Tensor((<span class="number">2</span>, <span class="number">4</span>)).to(torch.int32)</span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \</span><br><span class="line">                     (<span class="number">0</span>, <span class="built_in">max</span>(src_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]), <span class="number">2</span>)  <span class="comment"># 2*4*1</span></span><br><span class="line"><span class="comment"># 计算两个三维tensor的矩阵乘法，torch.bmm(a,b),(b,h,w)×(b,w,h)-&gt;(b,h,h)</span></span><br><span class="line">valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 2*4*4</span></span><br><span class="line">invalid_encoder_pos_matrix = <span class="number">1</span>-valid_encoder_pos_matrix</span><br><span class="line">mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size, <span class="built_in">max</span>(src_len), <span class="built_in">max</span>(src_len)) <span class="comment"># 2*4*4</span></span><br><span class="line">masked_score = score.masked_fill(mask_encoder_self_attention, -np.inf)</span><br><span class="line">prob = F.softmax(masked_score, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-5-intra-attention-mask"><a href="#2-5-intra-attention-mask" class="headerlink" title="2-5 intra-attention mask"></a>2-5 intra-attention mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">src_len = torch.Tensor((<span class="number">2</span>, <span class="number">4</span>)).to(torch.int32)</span><br><span class="line">tgt_len = torch.Tensor((<span class="number">4</span>, <span class="number">3</span>)).to(torch.int32)</span><br><span class="line"><span class="comment"># Q*K^T的shape：[batch_size, tgt_seq_len, src_seq_len]</span></span><br><span class="line">valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \</span><br><span class="line">                     (<span class="number">0</span>, <span class="built_in">max</span>(src_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> src_len]), <span class="number">2</span>)  <span class="comment"># 2*4*1 </span></span><br><span class="line">valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), \</span><br><span class="line">                     (<span class="number">0</span>, <span class="built_in">max</span>(tgt_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]), <span class="number">2</span>)  <span class="comment"># 2*4*1 </span></span><br><span class="line">valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 2*4*4</span></span><br><span class="line">invalid_cross_pos_matrix = <span class="number">1</span>-valid_cross_pos_matrix</span><br><span class="line">mask_cross_self_attention = invalid_cross_pos_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size, <span class="built_in">max</span>(tgt_len), <span class="built_in">max</span>(src_len)) <span class="comment"># 2*4*4</span></span><br><span class="line">masked_score = score.masked_fill(mask_cross_self_attention, -np.inf)</span><br><span class="line">prob = F.softmax(masked_score, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-6-decoder-self-attention-mask"><a href="#2-6-decoder-self-attention-mask" class="headerlink" title="2-6 decoder self_attention mask"></a>2-6 decoder self_attention mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">tgt_len = torch.Tensor((<span class="number">4</span>, <span class="number">3</span>)).to(torch.int32)</span><br><span class="line">valid_decoder_tri_matrix = torch.cat([torch.sequeeze(F.pad(torch.tril(torch.ones((L, L))), \</span><br><span class="line">                                    (<span class="number">0</span>, <span class="built_in">max</span>(tgt_len)-L), <span class="number">0</span>, <span class="built_in">max</span>(tgt_len)-L)), <span class="number">0</span>) <span class="keyword">for</span> L <span class="keyword">in</span> tgt_len]) <span class="comment"># 2*4*4</span></span><br><span class="line"></span><br><span class="line">invalid_decoder_tri_matrix = <span class="number">1</span>-valid_decoder_tri_matrix</span><br><span class="line">mask_decoder_self_attention = invalid_decoder_tri_matrix.to(torch.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">score = torch.randn(batch_size, <span class="built_in">max</span>(tgt_len), <span class="built_in">max</span>(tgt_len)) <span class="comment"># 2*4*4</span></span><br><span class="line">masked_score = score.masked_fill(mask_decoder_self_attention, -np.inf)</span><br><span class="line">prob = F.softmax(masked_score, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-7-scaled-dot-self-attention"><a href="#2-7-scaled-dot-self-attention" class="headerlink" title="2-7 scaled-dot self-attention"></a>2-7 scaled-dot self-attention</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q, K, V, attn_mask</span>):</span><br><span class="line">    <span class="comment"># shape of Q, K, V:(batch_size*num_head, seq_len, model_dim/num_head)</span></span><br><span class="line">    score = torch.bmm(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))/torch.sqrt(model.dim)</span><br><span class="line">    masked_score = score.masked_fill(attn_mask, -<span class="number">1e9</span>)</span><br><span class="line">    prob = F.softmax(masked_score, -<span class="number">1</span>)</span><br><span class="line">    context = torch.bmm(prob, V)</span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure>

<h3 id="3-Transformer模型总结"><a href="#3-Transformer模型总结" class="headerlink" title="3 Transformer模型总结"></a>3 Transformer模型总结</h3>

<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/01/12/hello-world/">Hello World</a></div></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="sitemap"><div class="sitemap-group"><span class="fs14">博客</span><a href="/about/">关于</a></div><div class="sitemap-group"><span class="fs14">文章</span><a href="/posts/">近期</a><a href="/categories/">分类</a><a href="/tags/">标签</a><a href="/archives/">归档</a></div><div class="sitemap-group"><span class="fs14">社交</span><a href="mailto:me@gdp_97@163.com">Email</a><a target="_blank" rel="noopener" href="https://github.com/gdp97">GitHub</a><a target="_blank" rel="noopener" href="https://space.bilibili.com/64771437">哔哩哔哩</a><a target="_blank" rel="noopener" href="https://music.163.com/#/user/home?id=347156020">网易云音乐</a></div></div><div class="text"><p>本站由 <a href="/">@anonymity</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.18.5';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
